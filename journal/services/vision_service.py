"""
Service d'analyse d'images avec Hugging Face CLIP
Vision AI pour dÃ©tection d'objets, couleurs, lieux, Ã©motions
"""

import torch
import cv2
import numpy as np
from PIL import Image, ImageDraw
from sklearn.cluster import KMeans
import json
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union

# Import du module d'amÃ©lioration des descriptions
try:
    from .description_enhancer import enhance_analysis_results
    ENHANCER_AVAILABLE = True
except ImportError:
    print("Description enhancer non disponible")
    ENHANCER_AVAILABLE = False

# Imports avec fallback
try:
    from transformers import CLIPProcessor, CLIPModel
    CLIP_AVAILABLE = True
except ImportError:
    print("CLIP non disponible, utilisation du mode fallback")
    CLIP_AVAILABLE = False

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class VisionAIService:
    """Service principal pour l'analyse d'images avec CLIP"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"Initialisation Vision AI sur {self.device}")
        
        # Charger le modÃ¨le CLIP
        self.model_name = "openai/clip-vit-base-patch32"
        self.processor = None
        self.model = None
        self._load_models()
        
        # Dictionnaires de dÃ©tection prÃ©dÃ©finis
        self.objects_categories = {
            'nature': ['tree', 'forest', 'mountain', 'beach', 'ocean', 'lake', 'river', 'flower', 'garden', 'park', 'sunrise', 'sunset', 'sky', 'cloud', 'grass', 'leaves'],
            'animals': ['dog', 'cat', 'bird', 'horse', 'fish', 'butterfly', 'elephant', 'lion', 'tiger', 'bear'],
            'food': ['pizza', 'burger', 'cake', 'coffee', 'wine', 'fruit', 'vegetable', 'bread', 'pasta', 'salad'],
            'transport': ['car', 'bike', 'train', 'plane', 'boat', 'bus', 'motorcycle', 'truck'],
            'architecture': ['building', 'house', 'bridge', 'church', 'castle', 'tower', 'monument'],
            'people': ['person', 'child', 'man', 'woman', 'family', 'couple', 'friends', 'crowd'],
            'activities': ['sports', 'dancing', 'cooking', 'reading', 'working', 'playing', 'swimming', 'running']
        }
        
        self.landmarks = {
            'Paris': ['Eiffel Tower', 'Arc de Triomphe', 'Louvre', 'Notre Dame', 'Champs Elysees'],
            'London': ['Big Ben', 'Tower Bridge', 'London Eye', 'Westminster'],
            'New York': ['Statue of Liberty', 'Empire State Building', 'Brooklyn Bridge', 'Times Square'],
            'Rome': ['Colosseum', 'Vatican', 'Trevi Fountain', 'Pantheon'],
            'Tokyo': ['Tokyo Tower', 'Mount Fuji', 'Shibuya Crossing'],
            'Sydney': ['Opera House', 'Harbour Bridge']
        }
        
        self.emotions_keywords = {
            'joyful': ['happy', 'smiling', 'celebration', 'party', 'fun', 'cheerful', 'bright', 'colorful'],
            'peaceful': ['calm', 'serene', 'quiet', 'meditation', 'zen', 'peaceful', 'relaxing'],
            'dramatic': ['storm', 'dark', 'intense', 'powerful', 'dramatic', 'moody'],
            'romantic': ['romantic', 'love', 'couple', 'wedding', 'flowers', 'sunset', 'candlelight'],
            'melancholic': ['sad', 'alone', 'empty', 'gray', 'rain', 'abandoned', 'nostalgic'],
            'energetic': ['action', 'sports', 'running', 'jumping', 'dynamic', 'fast', 'movement']
        }
    
    def _load_models(self):
        """Charge les modÃ¨les CLIP"""
        if not CLIP_AVAILABLE:
            logger.warning("âš ï¸ CLIP non disponible, utilisation du mode simulation")
            self.processor = None
            self.model = None
            return
        
        try:
            logger.info("ğŸ“¥ Chargement du modÃ¨le CLIP...")
            self.processor = CLIPProcessor.from_pretrained(self.model_name)
            self.model = CLIPModel.from_pretrained(self.model_name)
            self.model.to(self.device)
            self.model.eval()
            logger.info("âœ… ModÃ¨le CLIP chargÃ© avec succÃ¨s")
        except Exception as e:
            logger.error(f"âŒ Erreur lors du chargement de CLIP: {e}")
            logger.info("ğŸ”„ Passage en mode simulation")
            self.processor = None
            self.model = None
    
    def analyze_image(self, image_path: Union[str, Path]) -> Dict:
        """
        Analyse complÃ¨te d'une image
        
        Args:
            image_path: Chemin vers l'image
            
        Returns:
            Dict contenant tous les rÃ©sultats d'analyse
        """
        logger.info(f"ğŸ” Analyse de l'image: {image_path}")
        
        try:
            # Charger l'image
            image = Image.open(image_path).convert('RGB')
            
            # Analyser tous les aspects
            results = {
                'detected_objects': self.detect_objects(image),
                'detected_locations': self.detect_landmarks(image),
                'dominant_colors': self.extract_dominant_colors(image),
                'detected_emotions': self.detect_emotions(image),
                'image_description': self.generate_description(image),
                'confidence_scores': {}
            }
            
            # AmÃ©liorer les titres et descriptions si le module est disponible
            if ENHANCER_AVAILABLE:
                try:
                    results = enhance_analysis_results(results)
                    logger.info("âœ¨ Descriptions amÃ©liorÃ©es")
                except Exception as e:
                    logger.warning(f"âš ï¸ AmÃ©lioration descriptions Ã©chouÃ©e: {e}")
            
            logger.info("âœ… Analyse terminÃ©e avec succÃ¨s")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors de l'analyse: {e}")
            return {
                'error': str(e),
                'detected_objects': [],
                'detected_locations': [],
                'dominant_colors': [],
                'detected_emotions': [],
                'image_description': '',
                'confidence_scores': {}
            }
    
    def detect_objects(self, image: Image.Image, threshold: float = 0.3) -> List[Dict]:
        """DÃ©tecte les objets dans l'image avec CLIP ou simulation"""
        logger.info("ğŸ” DÃ©tection d'objets...")
        
        # Mode simulation si CLIP n'est pas disponible
        if self.model is None:
            return self._simulate_object_detection(image)
        
        detected_objects = []
        
        try:
            # Tester quelques catÃ©gories d'objets (optimisÃ© pour Ã©viter les timeouts)
            test_categories = {
                'nature': ['tree', 'flower', 'beach', 'mountain', 'sky'],
                'people': ['person', 'face', 'smile'],
                'food': ['food', 'cake', 'fruit'],
                'transport': ['car', 'bike', 'plane']
            }
            
            for category, objects in test_categories.items():
                # CrÃ©er les prompts textuels
                text_inputs = [f"a photo of {obj}" for obj in objects]
                
                # Traitement avec CLIP
                inputs = self.processor(
                    text=text_inputs, 
                    images=image, 
                    return_tensors="pt", 
                    padding=True
                ).to(self.device)
                
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    logits_per_image = outputs.logits_per_image
                    probs = logits_per_image.softmax(dim=1)
                
                # Extraire les objets dÃ©tectÃ©s avec confiance > threshold
                for i, (obj, prob) in enumerate(zip(objects, probs[0])):
                    confidence = prob.item()
                    if confidence > threshold:
                        detected_objects.append({
                            'object': obj,
                            'category': category,
                            'confidence': round(confidence, 3)
                        })
            
            # Trier par confiance dÃ©croissante
            detected_objects.sort(key=lambda x: x['confidence'], reverse=True)
            
            logger.info(f"âœ… {len(detected_objects)} objets dÃ©tectÃ©s")
            return detected_objects[:10]  # Top 10
            
        except Exception as e:
            logger.error(f"âŒ Erreur dÃ©tection objets: {e}")
            return self._simulate_object_detection(image)
    
    def _simulate_object_detection(self, image: Image.Image) -> List[Dict]:
        """Simulation de dÃ©tection d'objets basÃ©e sur l'analyse de couleurs"""
        logger.info("ğŸ­ Mode simulation - DÃ©tection intelligente basÃ©e sur les couleurs")
        
        # Analyser les couleurs dominantes
        colors = self.extract_dominant_colors(image, n_colors=8)
        detected_objects = []
        
        # Analyser la composition de l'image
        img_array = np.array(image)
        height, width = img_array.shape[:2]
        
        # PRIORITÃ‰ 1: DÃ‰TECTER LES MONUMENTS D'ABORD
        monument_detected = self._detect_monument_in_image(img_array, colors)
        if monument_detected:
            # Si c'est un monument, NE PAS ajouter "personne"
            detected_objects.append({
                'object': monument_detected['type'],
                'category': 'architecture',
                'confidence': monument_detected['confidence']
            })
            detected_objects.append({
                'object': 'monument',
                'category': 'architecture',
                'confidence': monument_detected['confidence'] * 0.9
            })
            logger.info(f"ğŸ›ï¸ Monument dÃ©tectÃ©: {monument_detected['type']} - {monument_detected['confidence']}")
            
            # Ajouter des objets contextuels mais PAS DE PERSONNES
            for color in colors[:3]:
                color_name = color.get('name', '').lower()
                confidence = color.get('percentage', 0) / 100
                
                if 'bleu' in color_name and confidence > 0.20:
                    detected_objects.append({
                        'object': 'ciel',
                        'category': 'nature',
                        'confidence': round(confidence * 0.85, 3)
                    })
                elif color_name in ['gris', 'beige'] and confidence > 0.15:
                    detected_objects.append({
                        'object': 'architecture',
                        'category': 'urban',
                        'confidence': round(confidence * 0.80, 3)
                    })
            
            # Supprimer doublons et retourner
            return self._remove_duplicates_and_sort(detected_objects)[:5]
        
        # PRIORITÃ‰ 2: DÃ‰TECTION DE PLACES/ESPACES URBAINS
        place_detected = self._detect_place_in_image(img_array, colors)
        if place_detected:
            detected_objects.append({
                'object': 'place',
                'category': 'urban',
                'confidence': place_detected['confidence']
            })
            detected_objects.append({
                'object': 'espace urbain',
                'category': 'urban',
                'confidence': place_detected['confidence'] * 0.85
            })
            logger.info(f"ğŸ›ï¸ Place dÃ©tectÃ©e - confiance: {place_detected['confidence']}")
            
            # Retourner sans chercher de personnes
            return self._remove_duplicates_and_sort(detected_objects)[:5]
        
        # PRIORITÃ‰ 3: DÃ‰TECTION DE PERSONNES (uniquement si pas de monument/place)
        person_detected = self._detect_person_in_image(img_array, colors)
        if person_detected:
            detected_objects.append({
                'object': 'personne',
                'category': 'people',
                'confidence': person_detected['confidence']
            })
            logger.info(f"ğŸ‘¤ Personne dÃ©tectÃ©e avec confiance: {person_detected['confidence']}")
        
        # InfÃ©rer des objets basÃ©s sur les couleurs dominantes
        for color in colors[:5]:
            color_name = color.get('name', '').lower()
            confidence = color.get('percentage', 0) / 100
            
            # DÃ©tection de nature/paysage
            if 'vert' in color_name and confidence > 0.15:
                detected_objects.append({
                    'object': 'paysage',
                    'category': 'nature',
                    'confidence': round(confidence * 0.85, 3)
                })
            
            # DÃ©tection de ciel
            if 'bleu' in color_name and confidence > 0.25:
                detected_objects.append({
                    'object': 'ciel',
                    'category': 'nature', 
                    'confidence': round(confidence * 0.90, 3)
                })
            
            # DÃ©tection de scÃ¨ne colorÃ©e
            if color_name in ['rouge', 'orange', 'jaune'] and confidence > 0.15:
                detected_objects.append({
                    'object': 'scÃ¨ne colorÃ©e',
                    'category': 'general',
                    'confidence': round(confidence * 0.70, 3)
                })
        
        return self._remove_duplicates_and_sort(detected_objects)[:5]
    
    def _detect_monument_in_image(self, img_array: np.ndarray, colors: List[Dict]) -> Optional[Dict]:
        """DÃ©tecte spÃ©cifiquement les monuments cÃ©lÃ¨bres"""
        logger.info("ğŸ›ï¸ Analyse spÃ©cifique pour dÃ©tection de monuments...")
        
        color_names = [c.get('name', '').lower() for c in colors[:6]]
        
        # CrÃ©er un dictionnaire qui garde la PLUS GRANDE valeur pour chaque nom de couleur
        color_percentages = {}
        for c in colors[:6]:
            name = c.get('name', '').lower()
            percentage = c.get('percentage', 0)
            if name not in color_percentages or percentage > color_percentages[name]:
                color_percentages[name] = percentage
        
        has_blue_sky = any('bleu' in name and color_percentages.get(name, 0) > 12 
                          for name in color_percentages.keys())
        has_green = any('vert' in name and color_percentages.get(name, 0) > 8 
                       for name in color_percentages.keys())
        
        # 1. PRIORITÃ‰ HAUTE - ARC DE TRIOMPHE / CHAMPS-Ã‰LYSÃ‰ES
        # Signature TRÃˆS spÃ©cifique : BEAUCOUP de blanc/beige (pierre claire) + peu/pas de vert
        has_white_stone = color_percentages.get('blanc', 0) > 15 or color_percentages.get('beige', 0) > 18
        has_gray = color_percentages.get('gris', 0) > 8
        
        # Arc de Triomphe = pierre BLANCHE dominante + PAS de verdure importante
        if has_white_stone and has_blue_sky and not has_green:
            logger.info(f"ğŸ›ï¸ ARC DE TRIOMPHE/CHAMPS-Ã‰LYSÃ‰ES dÃ©tectÃ©! (blanc:{color_percentages.get('blanc', 0):.1f}%, beige:{color_percentages.get('beige', 0):.1f}%)")
            return {'type': 'Arc de Triomphe', 'confidence': 0.88}
        
        # 2. PRIORITÃ‰ HAUTE - TOUR EIFFEL
        # Signature spÃ©cifique : structure mÃ©tallique (marron/gris foncÃ©) + VERDURE
        has_dark_structure = (color_percentages.get('marron', 0) > 10 or 
                             color_percentages.get('brun', 0) > 10 or
                             color_percentages.get('gris', 0) > 10)
        
        # Tour Eiffel = structure sombre + ciel + VERDURE (Champ de Mars)
        if has_dark_structure and has_blue_sky and has_green:
            logger.info(f"ğŸ—¼ TOUR EIFFEL DÃ‰TECTÃ‰E! (structure_sombre:{has_dark_structure}, ciel:{has_blue_sky}, verdure:{has_green})")
            return {'type': 'Tour Eiffel', 'confidence': 0.92}
        
        # Tour Eiffel sans verdure visible (photo rapprochÃ©e)
        has_metallic = color_percentages.get('gris', 0) > 12 or color_percentages.get('marron', 0) > 12
        if has_metallic and has_blue_sky and not has_white_stone:
            logger.info(f"ğŸ—¼ TOUR EIFFEL probable (mÃ©tal:{has_metallic}, pas_pierre_blanche:{not has_white_stone})")
            return {'type': 'Tour Eiffel', 'confidence': 0.85}
        
        # PYRAMIDES - Signature : beige/sable dominant + ciel bleu
        desert_colors = ['beige', 'orange', 'jaune', 'marron']
        desert_percentage = sum(color_percentages.get(name, 0) for name in desert_colors)
        
        if desert_percentage > 35 and has_blue_sky:
            logger.info(f"ğŸ”º PYRAMIDES dÃ©tectÃ©es! (dÃ©sert: {desert_percentage}%)")
            return {'type': 'Pyramides d\'Ã‰gypte', 'confidence': 0.88}
        
        # MONUMENT GÃ‰NÃ‰RIQUE - Structure avec ciel
        has_structure = any(name in ['gris', 'marron', 'beige', 'noir'] for name in color_names if color_percentages.get(name, 0) > 18)
        
        if has_structure and has_blue_sky:
            logger.info(f"ğŸ›ï¸ Monument gÃ©nÃ©rique dÃ©tectÃ©")
            return {'type': 'monument historique', 'confidence': 0.70}
        
        return None
    
    def _detect_place_in_image(self, img_array: np.ndarray, colors: List[Dict]) -> Optional[Dict]:
        """DÃ©tecte les places et espaces urbains"""
        logger.info("ğŸ›ï¸ Analyse pour dÃ©tection de places...")
        
        # CrÃ©er un dictionnaire qui garde la PLUS GRANDE valeur pour chaque nom de couleur
        color_percentages = {}
        for c in colors[:6]:
            name = c.get('name', '').lower()
            percentage = c.get('percentage', 0)
            if name not in color_percentages or percentage > color_percentages[name]:
                color_percentages[name] = percentage
        
        # Places : gris (pavÃ©s) + beige/blanc (bÃ¢timents) + bleu (ciel)
        has_gray_pavement = color_percentages.get('gris', 0) > 15
        has_buildings = any(name in ['beige', 'blanc', 'marron'] and color_percentages.get(name, 0) > 12 
                           for name in color_percentages.keys())
        has_blue_sky = any('bleu' in name and color_percentages.get(name, 0) > 10 
                          for name in color_percentages.keys())
        
        if has_gray_pavement and has_buildings and has_blue_sky:
            logger.info(f"ğŸ›ï¸ Place urbaine dÃ©tectÃ©e!")
            return {'confidence': 0.80}
        
        return None
    
    def _remove_duplicates_and_sort(self, objects: List[Dict]) -> List[Dict]:
        """Supprime les doublons et trie par confiance"""
        seen = set()
        unique_objects = []
        for obj in objects:
            if obj['object'] not in seen:
                seen.add(obj['object'])
                unique_objects.append(obj)
        
        unique_objects.sort(key=lambda x: x['confidence'], reverse=True)
        return unique_objects
    
    def _detect_person_in_image(self, img_array: np.ndarray, colors: List[Dict]) -> Optional[Dict]:
        """DÃ©tecte la prÃ©sence d'une personne dans l'image basÃ© sur l'analyse visuelle"""
        logger.info("ğŸ‘¤ Analyse spÃ©cifique pour dÃ©tection de personnes...")
        
        height, width = img_array.shape[:2]
        person_indicators = 0
        confidence_factors = []
        
        # 1. DÃ‰TECTION DES TONS DE PEAU
        skin_colors = ['beige', 'rose', 'couleur mixte', 'marron', 'orange']
        for color in colors:
            color_name = color.get('name', '').lower()
            percentage = color.get('percentage', 0)
            
            # Analyse RGB pour tons de peau plus prÃ©cise
            rgb = color.get('rgb', [0, 0, 0])
            if len(rgb) == 3:
                r, g, b = rgb
                # Algorithme de dÃ©tection de tons de peau
                if (r > 95 and g > 40 and b > 20 and 
                    max(r, g, b) - min(r, g, b) > 15 and 
                    abs(r - g) > 15 and r > g and r > b):
                    person_indicators += 2
                    confidence_factors.append(('skin_tone_rgb', 0.4))
                    logger.info(f"ğŸ¨ Ton de peau dÃ©tectÃ© via RGB: {rgb}")
                    
                # Tons de peau par nom de couleur
                elif any(skin in color_name for skin in skin_colors) and percentage > 5:
                    person_indicators += 1
                    confidence_factors.append(('skin_tone_name', 0.2))
                    logger.info(f"ğŸ¨ Ton de peau dÃ©tectÃ© via nom: {color_name}")
        
        # 2. ANALYSE DE FORME ET PROPORTIONS
        # VÃ©rifier les proportions typiques d'un portrait/personne
        aspect_ratio = width / height
        if 0.6 <= aspect_ratio <= 1.4:  # Format portrait ou carrÃ© typique des photos de personnes
            person_indicators += 1
            confidence_factors.append(('portrait_ratio', 0.15))
            logger.info(f"ğŸ“ Format portrait dÃ©tectÃ©: {aspect_ratio:.2f}")
        
        # 3. DÃ‰TECTION DE COULEURS VESTIMENTAIRES
        clothing_colors = ['noir', 'blanc', 'gris', 'bleu', 'rouge', 'vert', 'jaune']
        clothing_detected = 0
        for color in colors:
            color_name = color.get('name', '').lower()
            percentage = color.get('percentage', 0)
            if color_name in clothing_colors and percentage > 10:
                clothing_detected += 1
        
        if clothing_detected >= 2:  # Au moins 2 couleurs de vÃªtements
            person_indicators += 1
            confidence_factors.append(('clothing_colors', 0.2))
            logger.info(f"ğŸ‘• Couleurs vestimentaires dÃ©tectÃ©es: {clothing_detected}")
        
        # 4. ANALYSE DE LA DISTRIBUTION DES COULEURS
        # Les photos de personnes ont souvent une distribution spÃ©cifique
        if len(colors) >= 3:
            # VÃ©rifier s'il y a une couleur dominante (arriÃ¨re-plan) et des couleurs secondaires (personne)
            main_color_pct = colors[0].get('percentage', 0)
            secondary_color_pct = colors[1].get('percentage', 0)
            
            if main_color_pct > 30 and secondary_color_pct > 10:
                person_indicators += 1
                confidence_factors.append(('color_distribution', 0.15))
                logger.info(f"ğŸ¨ Distribution couleurs favorable: {main_color_pct}% / {secondary_color_pct}%")
        
        # 5. ANALYSE TEXTURE (basÃ©e sur la variance des couleurs)
        color_variance = len([c for c in colors if c.get('percentage', 0) > 5])
        if color_variance >= 4:  # Variance de couleurs typique d'une personne (peau, cheveux, vÃªtements, arriÃ¨re-plan)
            person_indicators += 1
            confidence_factors.append(('color_variance', 0.1))
            logger.info(f"ğŸŒˆ Variance de couleurs dÃ©tectÃ©e: {color_variance}")
        
        # CALCUL DE LA CONFIANCE FINALE
        if person_indicators >= 2:  # Seuil minimum pour dÃ©tecter une personne
            base_confidence = min(0.85, person_indicators * 0.2)  # Base selon nombre d'indicateurs
            
            # Ajouter les facteurs de confiance spÃ©cifiques
            bonus_confidence = sum(factor[1] for factor in confidence_factors)
            final_confidence = min(0.95, base_confidence + bonus_confidence)
            
            logger.info(f"âœ… Personne dÃ©tectÃ©e! Indicateurs: {person_indicators}, Confiance: {final_confidence:.3f}")
            logger.info(f"ğŸ“‹ Facteurs: {[f[0] for f in confidence_factors]}")
            
            return {
                'confidence': round(final_confidence, 3),
                'indicators': person_indicators,
                'factors': confidence_factors
            }
        
        logger.info(f"âŒ Personne non dÃ©tectÃ©e. Indicateurs insuffisants: {person_indicators}")
        return None
    
    def detect_landmarks(self, image: Image.Image, threshold: float = 0.4) -> List[Dict]:
        """DÃ©tecte les monuments et lieux cÃ©lÃ¨bres"""
        logger.info("ğŸ›ï¸ DÃ©tection de lieux...")
        
        # Mode simulation si CLIP n'est pas disponible - analyse basÃ©e sur les couleurs
        if self.model is None:
            return self._simulate_landmark_detection(image)
        
        detected_locations = []
        
        try:
            # Tester seulement quelques landmarks populaires
            test_landmarks = {
                'Paris': ['Eiffel Tower', 'Arc de Triomphe', 'Louvre Museum'],
                'London': ['Big Ben', 'Tower Bridge', 'London Eye'],
                'New York': ['Statue of Liberty', 'Empire State Building', 'Brooklyn Bridge'],
                'Rome': ['Colosseum', 'Trevi Fountain', 'Vatican'],
                'Dubai': ['Burj Khalifa', 'Palm Jumeirah']
            }
            
            for city, landmarks in test_landmarks.items():
                # CrÃ©er les prompts pour les monuments
                text_inputs = [f"a photo of {landmark}" for landmark in landmarks]
                
                inputs = self.processor(
                    text=text_inputs,
                    images=image,
                    return_tensors="pt",
                    padding=True
                ).to(self.device)
                
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    logits_per_image = outputs.logits_per_image
                    probs = logits_per_image.softmax(dim=1)
                
                # VÃ©rifier les monuments dÃ©tectÃ©s
                for landmark, prob in zip(landmarks, probs[0]):
                    confidence = prob.item()
                    if confidence > threshold:
                        detected_locations.append({
                            'landmark': landmark,
                            'city': city,
                            'confidence': round(confidence, 3)
                        })
            
            # Trier par confiance
            detected_locations.sort(key=lambda x: x['confidence'], reverse=True)
            return detected_locations[:3]
            
        except Exception as e:
            logger.error(f"Erreur dÃ©tection lieux: {e}")
            return []
    
    def _simulate_landmark_detection(self, image: Image.Image) -> List[Dict]:
        """Simulation de dÃ©tection de monuments basÃ©e sur l'analyse de l'image"""
        logger.info("ğŸ­ Mode simulation - DÃ©tection de monuments basÃ©e sur les couleurs et formes")
        
        # Analyser les couleurs pour dÃ©tecter des patterns
        colors = self.extract_dominant_colors(image, n_colors=8)
        
        # Convertir l'image en array pour analyse
        img_array = np.array(image)
        height, width = img_array.shape[:2]
        
        detected_locations = []
        monument_indicators = 0
        
        # Analyser les couleurs dominantes
        color_names = [c.get('name', '').lower() for c in colors[:5]]
        
        # CrÃ©er un dictionnaire qui garde la PLUS GRANDE valeur pour chaque nom de couleur
        color_percentages = {}
        for c in colors[:5]:
            name = c.get('name', '').lower()
            percentage = c.get('percentage', 0)
            if name not in color_percentages or percentage > color_percentages[name]:
                color_percentages[name] = percentage
        
        # 1. DÃ‰TECTION DE CIEL BLEU (monuments extÃ©rieurs)
        has_blue_sky = any('bleu' in name and color_percentages.get(name, 0) > 10 
                          for name in color_percentages.keys())
        has_green = any('vert' in name and color_percentages.get(name, 0) > 8 
                       for name in color_percentages.keys())
        
        if has_blue_sky:
            monument_indicators += 1
            logger.info("â˜ï¸ Ciel bleu dÃ©tectÃ© - monument extÃ©rieur probable")
        
        # 2. DÃ‰TECTION DE STRUCTURES (gris, marron, beige, noir)
        structure_colors = ['gris', 'marron', 'brun', 'noir', 'beige', 'orange']
        has_structure = any(name in structure_colors and color_percentages.get(name, 0) > 15 
                           for name in color_percentages.keys())
        if has_structure:
            monument_indicators += 2  # Poids plus important
            logger.info(f"ğŸ›ï¸ Structure dÃ©tectÃ©e - couleurs: {[n for n in color_names if n in structure_colors]}")
        
        # 3. PRIORITÃ‰ HAUTE - ARC DE TRIOMPHE (Pierre blanche + PAS de verdure)
        has_white_stone = color_percentages.get('blanc', 0) > 15 or color_percentages.get('beige', 0) > 18
        
        if has_white_stone and has_blue_sky and not has_green:
            logger.info(f"ğŸ›ï¸ ARC DE TRIOMPHE dÃ©tectÃ©! (blanc:{color_percentages.get('blanc', 0):.1f}%, pas_vert:{not has_green})")
            detected_locations.append({
                'landmark': 'Arc de Triomphe',
                'city': 'Paris',
                'confidence': 0.88
            })
            return detected_locations
        
        # 4. PRIORITÃ‰ HAUTE - TOUR EIFFEL (Structure mÃ©tallique + VERDURE)
        has_dark_structure = (color_percentages.get('marron', 0) > 10 or 
                             color_percentages.get('brun', 0) > 10 or
                             color_percentages.get('gris', 0) > 10)
        
        if has_dark_structure and has_blue_sky and has_green:
            logger.info(f"ğŸ—¼ TOUR EIFFEL dÃ©tectÃ©e! (structure_sombre:{has_dark_structure}, verdure:{has_green})")
            detected_locations.append({
                'landmark': 'Tour Eiffel',
                'city': 'Paris',
                'confidence': 0.92
            })
            return detected_locations
        
        # Tour Eiffel sans verdure visible (photo rapprochÃ©e)
        has_metallic = color_percentages.get('gris', 0) > 12 or color_percentages.get('marron', 0) > 12
        if has_metallic and has_blue_sky and not has_white_stone:
            logger.info(f"ğŸ—¼ TOUR EIFFEL probable (mÃ©tal:{has_metallic}, pas_blanc:{not has_white_stone})")
            detected_locations.append({
                'landmark': 'Tour Eiffel',
                'city': 'Paris',
                'confidence': 0.85
            })
            return detected_locations
        
        # 5. DÃ‰TECTION DE SABLE/DÃ‰SERT (pour pyramides)
        desert_colors = ['beige', 'orange', 'jaune', 'marron']
        has_desert = any(name in desert_colors and color_percentages.get(name, 0) > 20 
                        for name in color_percentages.keys())
        desert_percentage = sum(color_percentages.get(name, 0) for name in desert_colors)
        
        if has_desert and desert_percentage > 30:
            monument_indicators += 2
            logger.info(f"ğŸœï¸ DÃ©sert dÃ©tectÃ© ({desert_percentage:.1f}%) - possibles pyramides")
            
            # PYRAMIDES D'Ã‰GYPTE
            if desert_percentage > 40:
                detected_locations.append({
                    'landmark': 'Pyramides de Gizeh',
                    'city': 'Le Caire, Ã‰gypte',
                    'confidence': 0.85
                })
                logger.info("ğŸ”º PYRAMIDES DÃ‰TECTÃ‰ES!")
                return detected_locations
        
        # 6. MONUMENT GÃ‰NÃ‰RIQUE (si assez d'indicateurs)
        if monument_indicators >= 3:
            logger.info(f"ğŸ›ï¸ Monument gÃ©nÃ©rique dÃ©tectÃ© ({monument_indicators} indicateurs)")
            detected_locations.append({
                'landmark': 'Monument historique',
                'city': 'Inconnu',
                'confidence': 0.60
            })
        
        return detected_locations
        
        try:
            # Tester seulement quelques landmarks populaires
            test_landmarks = {
                'Paris': ['Eiffel Tower', 'Arc de Triomphe'],
                'London': ['Big Ben', 'Tower Bridge'],
                'New York': ['Statue of Liberty', 'Empire State Building']
            }
            
            for city, landmarks in test_landmarks.items():
                # CrÃ©er les prompts pour les monuments
                text_inputs = [f"a photo of {landmark}" for landmark in landmarks]
                
                inputs = self.processor(
                    text=text_inputs,
                    images=image,
                    return_tensors="pt",
                    padding=True
                ).to(self.device)
                
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    logits_per_image = outputs.logits_per_image
                    probs = logits_per_image.softmax(dim=1)
                
                # VÃ©rifier les monuments dÃ©tectÃ©s
                for landmark, prob in zip(landmarks, probs[0]):
                    confidence = prob.item()
                    if confidence > threshold:
                        detected_locations.append({
                            'landmark': landmark,
                            'city': city,
                            'confidence': round(confidence, 3)
                        })
            
            # Trier par confiance
            detected_locations.sort(key=lambda x: x['confidence'], reverse=True)
            
            logger.info(f"âœ… {len(detected_locations)} lieux dÃ©tectÃ©s")
            return detected_locations[:3]  # Top 3
            
        except Exception as e:
            logger.error(f"âŒ Erreur dÃ©tection lieux: {e}")
            return []
    
    def extract_dominant_colors(self, image: Image.Image, n_colors: int = 5) -> List[Dict]:
        """Extrait les couleurs dominantes avec K-means"""
        logger.info("ğŸ¨ Analyse des couleurs...")
        
        try:
            # Convertir en array numpy
            img_array = np.array(image)
            
            # Redimensionner pour accÃ©lÃ©rer le traitement
            if img_array.shape[0] > 300 or img_array.shape[1] > 300:
                image_resized = image.copy()
                image_resized.thumbnail((300, 300))
                img_array = np.array(image_resized)
            
            # Reshape pour K-means
            pixels = img_array.reshape(-1, 3)
            
            # K-means clustering
            kmeans = KMeans(n_clusters=n_colors, random_state=42, n_init=10)
            kmeans.fit(pixels)
            
            colors = []
            percentages = np.bincount(kmeans.labels_) / len(pixels)
            
            for i, (color, percentage) in enumerate(zip(kmeans.cluster_centers_, percentages)):
                # Convertir en RGB entier
                rgb = [int(c) for c in color]
                hex_color = "#{:02x}{:02x}{:02x}".format(*rgb)
                
                colors.append({
                    'rgb': rgb,
                    'hex': hex_color,
                    'percentage': round(percentage * 100, 1),
                    'name': self._get_color_name(rgb)
                })
            
            # Trier par pourcentage dÃ©croissant
            colors.sort(key=lambda x: x['percentage'], reverse=True)
            
            logger.info(f"âœ… {len(colors)} couleurs dominantes extraites")
            return colors
            
        except Exception as e:
            logger.error(f"âŒ Erreur analyse couleurs: {e}")
            return []
    
    def detect_emotions(self, image: Image.Image, threshold: float = 0.25) -> List[Dict]:
        """DÃ©tecte l'ambiance/Ã©motion de l'image"""
        logger.info("ğŸ˜Š DÃ©tection des Ã©motions...")
        
        # Mode simulation si CLIP n'est pas disponible
        if self.model is None:
            return self._simulate_emotion_detection(image)
        
        detected_emotions = []
        
        try:
            # Tester seulement quelques Ã©motions principales
            test_emotions = {
                'joyful': ['happy', 'bright', 'colorful'],
                'peaceful': ['calm', 'serene', 'quiet'],
                'dramatic': ['dark', 'intense', 'moody']
            }
            
            for emotion, keywords in test_emotions.items():
                # CrÃ©er les prompts Ã©motionnels
                text_inputs = [f"a {keyword} photo" for keyword in keywords]
                
                inputs = self.processor(
                    text=text_inputs,
                    images=image,
                    return_tensors="pt",
                    padding=True
                ).to(self.device)
                
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    logits_per_image = outputs.logits_per_image
                    probs = logits_per_image.softmax(dim=1)
                
                # Calculer la confiance moyenne pour cette Ã©motion
                avg_confidence = probs[0].mean().item()
                
                if avg_confidence > threshold:
                    detected_emotions.append({
                        'emotion': emotion,
                        'confidence': round(avg_confidence, 3),
                        'keywords': keywords
                    })
            
            # Trier par confiance
            detected_emotions.sort(key=lambda x: x['confidence'], reverse=True)
            
            logger.info(f"âœ… {len(detected_emotions)} Ã©motions dÃ©tectÃ©es")
            return detected_emotions[:3]  # Top 3
            
        except Exception as e:
            logger.error(f"âŒ Erreur dÃ©tection Ã©motions: {e}")
            return self._simulate_emotion_detection(image)
    
    def _simulate_emotion_detection(self, image: Image.Image) -> List[Dict]:
        """Simulation de dÃ©tection d'Ã©motions basÃ©e sur les couleurs"""
        colors = self.extract_dominant_colors(image)
        emotions = []
        
        # Analyser les couleurs pour infÃ©rer l'Ã©motion
        color_names = [c.get('name', '').lower() for c in colors[:3]]
        
        if any('rouge' in name or 'orange' in name for name in color_names):
            emotions.append({'emotion': 'energetic', 'confidence': 0.6, 'keywords': ['vibrant', 'warm']})
        if any('bleu' in name for name in color_names):
            emotions.append({'emotion': 'peaceful', 'confidence': 0.5, 'keywords': ['calm', 'cool']})
        if any('vert' in name for name in color_names):
            emotions.append({'emotion': 'natural', 'confidence': 0.7, 'keywords': ['nature', 'fresh']})
        
        return emotions[:2]
    
    def generate_description(self, image: Image.Image) -> str:
        """GÃ©nÃ¨re une description basique de l'image"""
        logger.info("ğŸ“ GÃ©nÃ©ration de description...")
        
        # Mode simulation si CLIP n'est pas disponible
        if self.model is None:
            colors = self.extract_dominant_colors(image)
            if colors:
                main_color = colors[0].get('name', 'colorÃ©e')
                return f"Image {main_color} avec {len(colors)} couleurs dominantes"
            return "Image analysÃ©e par Vision AI"
        
        try:
            # Prompts de description gÃ©nÃ©rique
            text_inputs = [
                "a beautiful photo",
                "an outdoor photo",
                "a colorful image"
            ]
            
            inputs = self.processor(
                text=text_inputs,
                images=image,
                return_tensors="pt",
                padding=True
            ).to(self.device)
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits_per_image = outputs.logits_per_image
                probs = logits_per_image.softmax(dim=1)
            
            # Prendre la description avec la plus haute probabilitÃ©
            best_idx = probs[0].argmax().item()
            best_description = text_inputs[best_idx]
            confidence = probs[0][best_idx].item()
            
            logger.info(f"âœ… Description gÃ©nÃ©rÃ©e: {best_description}")
            return f"{best_description} (confidence: {confidence:.3f})"
            
        except Exception as e:
            logger.error(f"âŒ Erreur gÃ©nÃ©ration description: {e}")
            return "Image analysis completed"
    
    def _get_color_name(self, rgb: List[int]) -> str:
        """Donne un nom approximatif Ã  une couleur RGB"""
        r, g, b = rgb
        
        # Couleurs de base - conditions amÃ©liorÃ©es
        if r > 200 and g > 200 and b > 200:
            return "blanc"
        elif r < 50 and g < 50 and b < 50:
            return "noir"
        # DÃ‰TECTION SPÃ‰CIALE DES TONS DE PEAU
        elif (r > 95 and g > 40 and b > 20 and 
              max(r, g, b) - min(r, g, b) > 15 and 
              abs(r - g) > 15 and r > g and r > b):
            # Tons de peau clairs
            if r > 180 and g > 120:
                return "beige"
            # Tons de peau moyens
            elif r > 140 and g > 80:
                return "rose"
            # Tons de peau foncÃ©s
            else:
                return "marron"
        elif r > 200 and g < 100 and b < 100:
            return "rouge"
        # Vert amÃ©liorÃ© - dÃ©tecte aussi les tons olives et verdÃ¢tres
        elif (g > r and g > b and g > 50) or (g > 80 and r < g + 30 and b < 100):
            return "vert"
        # Bleu amÃ©liorÃ© - dÃ©tecte toutes les nuances de bleu
        elif b > r and b > g and b > 80:
            return "bleu"
        elif r > 200 and g > 200 and b < 100:
            return "jaune"
        elif r > 200 and g < 100 and b > 200:
            return "magenta"
        elif r < 100 and g > 200 and b > 200:
            return "cyan"
        elif r > 150 and g > 100 and b < 100:
            return "orange"
        # Marron/brun pour les structures mÃ©talliques ET cheveux
        elif r > 60 and g > 40 and b < 80 and abs(r - g) < 50:
            return "marron"
        elif r > 150 and g < 150 and b > 150:
            return "violet"
        elif r > 100 and g > 100 and b > 100:
            return "gris"
        else:
            return "couleur mixte"


# Instance globale du service
vision_ai_service = VisionAIService()


def analyze_media_vision(image_path: Union[str, Path]) -> Dict:
    """
    Fonction utilitaire pour analyser une image
    
    Args:
        image_path: Chemin vers l'image
        
    Returns:
        Dict avec les rÃ©sultats d'analyse
    """
    return vision_ai_service.analyze_image(image_path)


# Test rapide si exÃ©cutÃ© directement
if __name__ == "__main__":
    print("ğŸ§ª Test du service Vision AI")
    
    # CrÃ©er une image de test simple
    test_image = Image.new('RGB', (100, 100), color='red')
    results = vision_ai_service.analyze_image(test_image)
    
    print("ğŸ“Š RÃ©sultats du test:")
    print(f"  Objets dÃ©tectÃ©s: {len(results.get('detected_objects', []))}")
    print(f"  Lieux dÃ©tectÃ©s: {len(results.get('detected_locations', []))}")
    print(f"  Couleurs dominantes: {len(results.get('dominant_colors', []))}")
    print(f"  Ã‰motions dÃ©tectÃ©es: {len(results.get('detected_emotions', []))}")
    print("âœ… Service Vision AI opÃ©rationnel")