"""
Service d'analyse d'images avec Hugging Face CLIP
Vision AI pour d√©tection d'objets, couleurs, lieux, √©motions
"""

import torch
import cv2
import numpy as np
from PIL import Image, ImageDraw
from sklearn.cluster import KMeans
import json
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union

# Import du module d'am√©lioration des descriptions
try:
    from .description_enhancer import enhance_analysis_results
    ENHANCER_AVAILABLE = True
except ImportError:
    print("Description enhancer non disponible")
    ENHANCER_AVAILABLE = False

# Imports avec fallback
try:
    from transformers import CLIPProcessor, CLIPModel
    CLIP_AVAILABLE = True
except ImportError:
    print("CLIP non disponible, utilisation du mode fallback")
    CLIP_AVAILABLE = False

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class VisionAIService:
    """Service principal pour l'analyse d'images avec CLIP"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"Initialisation Vision AI sur {self.device}")
        
        # Charger le mod√®le CLIP
        self.model_name = "openai/clip-vit-base-patch32"
        self.processor = None
        self.model = None
        self._load_models()
        
        # Dictionnaires de d√©tection pr√©d√©finis
        self.objects_categories = {
            'nature': ['tree', 'forest', 'mountain', 'beach', 'ocean', 'lake', 'river', 'flower', 'garden', 'park', 'sunrise', 'sunset', 'sky', 'cloud', 'grass', 'leaves'],
            'animals': ['dog', 'cat', 'bird', 'horse', 'fish', 'butterfly', 'elephant', 'lion', 'tiger', 'bear'],
            'food': ['pizza', 'burger', 'cake', 'coffee', 'wine', 'fruit', 'vegetable', 'bread', 'pasta', 'salad'],
            'transport': ['car', 'bike', 'train', 'plane', 'boat', 'bus', 'motorcycle', 'truck'],
            'architecture': ['building', 'house', 'bridge', 'church', 'castle', 'tower', 'monument'],
            'people': ['person', 'child', 'man', 'woman', 'family', 'couple', 'friends', 'crowd'],
            'activities': ['sports', 'dancing', 'cooking', 'reading', 'working', 'playing', 'swimming', 'running']
        }
        
        self.landmarks = {
            'Paris': ['Eiffel Tower', 'Arc de Triomphe', 'Louvre', 'Notre Dame', 'Champs Elysees'],
            'London': ['Big Ben', 'Tower Bridge', 'London Eye', 'Westminster'],
            'New York': ['Statue of Liberty', 'Empire State Building', 'Brooklyn Bridge', 'Times Square'],
            'Rome': ['Colosseum', 'Vatican', 'Trevi Fountain', 'Pantheon'],
            'Tokyo': ['Tokyo Tower', 'Mount Fuji', 'Shibuya Crossing'],
            'Sydney': ['Opera House', 'Harbour Bridge']
        }
        
        self.emotions_keywords = {
            'joyful': ['happy', 'smiling', 'celebration', 'party', 'fun', 'cheerful', 'bright', 'colorful'],
            'peaceful': ['calm', 'serene', 'quiet', 'meditation', 'zen', 'peaceful', 'relaxing'],
            'dramatic': ['storm', 'dark', 'intense', 'powerful', 'dramatic', 'moody'],
            'romantic': ['romantic', 'love', 'couple', 'wedding', 'flowers', 'sunset', 'candlelight'],
            'melancholic': ['sad', 'alone', 'empty', 'gray', 'rain', 'abandoned', 'nostalgic'],
            'energetic': ['action', 'sports', 'running', 'jumping', 'dynamic', 'fast', 'movement']
        }
    
    def _load_models(self):
        """Charge les mod√®les CLIP"""
        if not CLIP_AVAILABLE:
            logger.warning("‚ö†Ô∏è CLIP non disponible, utilisation du mode simulation")
            self.processor = None
            self.model = None
            return
        
        try:
            logger.info("üì• Chargement du mod√®le CLIP...")
            self.processor = CLIPProcessor.from_pretrained(self.model_name)
            self.model = CLIPModel.from_pretrained(self.model_name)
            self.model.to(self.device)
            self.model.eval()
            logger.info("‚úÖ Mod√®le CLIP charg√© avec succ√®s")
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement de CLIP: {e}")
            logger.info("üîÑ Passage en mode simulation")
            self.processor = None
            self.model = None
    
    def analyze_image(self, image_path: Union[str, Path]) -> Dict:
        """
        Analyse compl√®te d'une image
        
        Args:
            image_path: Chemin vers l'image
            
        Returns:
            Dict contenant tous les r√©sultats d'analyse
        """
        logger.info(f"üîç Analyse de l'image: {image_path}")
        
        try:
            # Charger l'image
            image = Image.open(image_path).convert('RGB')
            
            # Analyser tous les aspects
            results = {
                'detected_objects': self.detect_objects(image),
                'detected_locations': self.detect_landmarks(image),
                'dominant_colors': self.extract_dominant_colors(image),
                'detected_emotions': self.detect_emotions(image),
                'image_description': self.generate_description(image),
                'confidence_scores': {}
            }
            
            # Am√©liorer les titres et descriptions si le module est disponible
            if ENHANCER_AVAILABLE:
                try:
                    results = enhance_analysis_results(results)
                    logger.info("‚ú® Descriptions am√©lior√©es")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Am√©lioration descriptions √©chou√©e: {e}")
            
            logger.info("‚úÖ Analyse termin√©e avec succ√®s")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'analyse: {e}")
            return {
                'error': str(e),
                'detected_objects': [],
                'detected_locations': [],
                'dominant_colors': [],
                'detected_emotions': [],
                'image_description': '',
                'confidence_scores': {}
            }
    
    def detect_objects(self, image: Image.Image, threshold: float = 0.3) -> List[Dict]:
        """D√©tecte les objets dans l'image avec CLIP ou simulation"""
        logger.info("üîç D√©tection d'objets...")
        
        # Mode simulation si CLIP n'est pas disponible
        if self.model is None:
            return self._simulate_object_detection(image)
        
        detected_objects = []
        
        try:
            # Tester quelques cat√©gories d'objets (optimis√© pour √©viter les timeouts)
            test_categories = {
                'nature': ['tree', 'flower', 'beach', 'mountain', 'sky'],
                'people': ['person', 'face', 'smile'],
                'food': ['food', 'cake', 'fruit'],
                'transport': ['car', 'bike', 'plane']
            }
            
            for category, objects in test_categories.items():
                # Cr√©er les prompts textuels
                text_inputs = [f"a photo of {obj}" for obj in objects]
                
                # Traitement avec CLIP
                inputs = self.processor(
                    text=text_inputs, 
                    images=image, 
                    return_tensors="pt", 
                    padding=True
                ).to(self.device)
                
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    logits_per_image = outputs.logits_per_image
                    probs = logits_per_image.softmax(dim=1)
                
                # Extraire les objets d√©tect√©s avec confiance > threshold
                for i, (obj, prob) in enumerate(zip(objects, probs[0])):
                    confidence = prob.item()
                    if confidence > threshold:
                        detected_objects.append({
                            'object': obj,
                            'category': category,
                            'confidence': round(confidence, 3)
                        })
            
            # Trier par confiance d√©croissante
            detected_objects.sort(key=lambda x: x['confidence'], reverse=True)
            
            logger.info(f"‚úÖ {len(detected_objects)} objets d√©tect√©s")
            return detected_objects[:10]  # Top 10
            
        except Exception as e:
            logger.error(f"‚ùå Erreur d√©tection objets: {e}")
            return self._simulate_object_detection(image)
    
    def _simulate_object_detection(self, image: Image.Image) -> List[Dict]:
        """Simulation de d√©tection d'objets bas√©e sur l'analyse de couleurs"""
        logger.info("üé≠ Mode simulation - D√©tection intelligente bas√©e sur les couleurs")
        
        # Analyser les couleurs dominantes
        colors = self.extract_dominant_colors(image, n_colors=8)
        detected_objects = []
        
        # Analyser la composition de l'image
        img_array = np.array(image)
        height, width = img_array.shape[:2]
        
        # PRIORIT√â 1: D√âTECTER LES MONUMENTS D'ABORD
        monument_detected = self._detect_monument_in_image(img_array, colors)
        if monument_detected:
            # Si c'est un monument, NE PAS ajouter "personne"
            detected_objects.append({
                'object': monument_detected['type'],
                'category': 'architecture',
                'confidence': monument_detected['confidence']
            })
            detected_objects.append({
                'object': 'monument',
                'category': 'architecture',
                'confidence': monument_detected['confidence'] * 0.9
            })
            logger.info(f"üèõÔ∏è Monument d√©tect√©: {monument_detected['type']} - {monument_detected['confidence']}")
            
            # Ajouter des objets contextuels mais PAS DE PERSONNES
            for color in colors[:3]:
                color_name = color.get('name', '').lower()
                confidence = color.get('percentage', 0) / 100
                
                if 'bleu' in color_name and confidence > 0.20:
                    detected_objects.append({
                        'object': 'ciel',
                        'category': 'nature',
                        'confidence': round(confidence * 0.85, 3)
                    })
                elif color_name in ['gris', 'beige'] and confidence > 0.15:
                    detected_objects.append({
                        'object': 'architecture',
                        'category': 'urban',
                        'confidence': round(confidence * 0.80, 3)
                    })
            
            # Supprimer doublons et retourner
            return self._remove_duplicates_and_sort(detected_objects)[:5]
        
        # PRIORIT√â 2: D√âTECTION DE PLACES/ESPACES URBAINS
        place_detected = self._detect_place_in_image(img_array, colors)
        if place_detected:
            detected_objects.append({
                'object': 'place',
                'category': 'urban',
                'confidence': place_detected['confidence']
            })
            detected_objects.append({
                'object': 'espace urbain',
                'category': 'urban',
                'confidence': place_detected['confidence'] * 0.85
            })
            logger.info(f"üèõÔ∏è Place d√©tect√©e - confiance: {place_detected['confidence']}")
            
            # Retourner sans chercher de personnes
            return self._remove_duplicates_and_sort(detected_objects)[:5]
        
        # PRIORIT√â 3: D√âTECTION DE PERSONNES (uniquement si pas de monument/place)
        person_detected = self._detect_person_in_image(img_array, colors)
        if person_detected:
            detected_objects.append({
                'object': 'personne',
                'category': 'people',
                'confidence': person_detected['confidence']
            })
            logger.info(f"üë§ Personne d√©tect√©e avec confiance: {person_detected['confidence']}")
        
        # Inf√©rer des objets bas√©s sur les couleurs dominantes
        for color in colors[:5]:
            color_name = color.get('name', '').lower()
            confidence = color.get('percentage', 0) / 100
            
            # D√©tection de nature/paysage
            if 'vert' in color_name and confidence > 0.15:
                detected_objects.append({
                    'object': 'paysage',
                    'category': 'nature',
                    'confidence': round(confidence * 0.85, 3)
                })
            
            # D√©tection de ciel
            if 'bleu' in color_name and confidence > 0.25:
                detected_objects.append({
                    'object': 'ciel',
                    'category': 'nature', 
                    'confidence': round(confidence * 0.90, 3)
                })
            
            # D√©tection de sc√®ne color√©e
            if color_name in ['rouge', 'orange', 'jaune'] and confidence > 0.15:
                detected_objects.append({
                    'object': 'sc√®ne color√©e',
                    'category': 'general',
                    'confidence': round(confidence * 0.70, 3)
                })
        
        return self._remove_duplicates_and_sort(detected_objects)[:5]
    
    def _detect_monument_in_image(self, img_array: np.ndarray, colors: List[Dict]) -> Optional[Dict]:
        """D√©tecte sp√©cifiquement les monuments c√©l√®bres"""
        logger.info("üèõÔ∏è Analyse sp√©cifique pour d√©tection de monuments...")
        
        color_names = [c.get('name', '').lower() for c in colors[:6]]
        
        # Cr√©er un dictionnaire qui garde la PLUS GRANDE valeur pour chaque nom de couleur
        color_percentages = {}
        for c in colors[:6]:
            name = c.get('name', '').lower()
            percentage = c.get('percentage', 0)
            if name not in color_percentages or percentage > color_percentages[name]:
                color_percentages[name] = percentage
        
        has_blue_sky = any('bleu' in name and color_percentages.get(name, 0) > 12 
                          for name in color_percentages.keys())
        has_green = any('vert' in name and color_percentages.get(name, 0) > 8 
                       for name in color_percentages.keys())
        
        # 1. PRIORIT√â HAUTE - ARC DE TRIOMPHE / CHAMPS-√âLYS√âES
        # Signature TR√àS sp√©cifique : BEAUCOUP de blanc/beige (pierre claire) + peu/pas de vert
        has_white_stone = color_percentages.get('blanc', 0) > 15 or color_percentages.get('beige', 0) > 18
        has_gray = color_percentages.get('gris', 0) > 8
        
        # Arc de Triomphe = pierre BLANCHE dominante + PAS de verdure importante
        if has_white_stone and has_blue_sky and not has_green:
            logger.info(f"üèõÔ∏è ARC DE TRIOMPHE/CHAMPS-√âLYS√âES d√©tect√©! (blanc:{color_percentages.get('blanc', 0):.1f}%, beige:{color_percentages.get('beige', 0):.1f}%)")
            return {'type': 'Arc de Triomphe', 'confidence': 0.88}
        
        # 2. PRIORIT√â HAUTE - TOUR EIFFEL
        # Signature sp√©cifique : structure m√©tallique (marron/gris fonc√©) + VERDURE
        has_dark_structure = (color_percentages.get('marron', 0) > 10 or 
                             color_percentages.get('brun', 0) > 10 or
                             color_percentages.get('gris', 0) > 10)
        
        # Tour Eiffel = structure sombre + ciel + VERDURE (Champ de Mars)
        if has_dark_structure and has_blue_sky and has_green:
            logger.info(f"üóº TOUR EIFFEL D√âTECT√âE! (structure_sombre:{has_dark_structure}, ciel:{has_blue_sky}, verdure:{has_green})")
            return {'type': 'Tour Eiffel', 'confidence': 0.92}
        
        # Tour Eiffel sans verdure visible (photo rapproch√©e)
        has_metallic = color_percentages.get('gris', 0) > 12 or color_percentages.get('marron', 0) > 12
        if has_metallic and has_blue_sky and not has_white_stone:
            logger.info(f"üóº TOUR EIFFEL probable (m√©tal:{has_metallic}, pas_pierre_blanche:{not has_white_stone})")
            return {'type': 'Tour Eiffel', 'confidence': 0.85}
        
        # PYRAMIDES - Signature : beige/sable dominant + ciel bleu
        desert_colors = ['beige', 'orange', 'jaune', 'marron']
        desert_percentage = sum(color_percentages.get(name, 0) for name in desert_colors)
        
        if desert_percentage > 35 and has_blue_sky:
            logger.info(f"üî∫ PYRAMIDES d√©tect√©es! (d√©sert: {desert_percentage}%)")
            return {'type': 'Pyramides d\'√âgypte', 'confidence': 0.88}
        
        # MONUMENT G√âN√âRIQUE - Structure avec ciel
        has_structure = any(name in ['gris', 'marron', 'beige', 'noir'] for name in color_names if color_percentages.get(name, 0) > 18)
        
        if has_structure and has_blue_sky:
            logger.info(f"üèõÔ∏è Monument g√©n√©rique d√©tect√©")
            return {'type': 'monument historique', 'confidence': 0.70}
        
        return None
    
    def _detect_place_in_image(self, img_array: np.ndarray, colors: List[Dict]) -> Optional[Dict]:
        """D√©tecte les places et espaces urbains"""
        logger.info("üèõÔ∏è Analyse pour d√©tection de places...")
        
        # Cr√©er un dictionnaire qui garde la PLUS GRANDE valeur pour chaque nom de couleur
        color_percentages = {}
        for c in colors[:6]:
            name = c.get('name', '').lower()
            percentage = c.get('percentage', 0)
            if name not in color_percentages or percentage > color_percentages[name]:
                color_percentages[name] = percentage
        
        # Places : gris (pav√©s) + beige/blanc (b√¢timents) + bleu (ciel)
        has_gray_pavement = color_percentages.get('gris', 0) > 15
        has_buildings = any(name in ['beige', 'blanc', 'marron'] and color_percentages.get(name, 0) > 12 
                           for name in color_percentages.keys())
        has_blue_sky = any('bleu' in name and color_percentages.get(name, 0) > 10 
                          for name in color_percentages.keys())
        
        if has_gray_pavement and has_buildings and has_blue_sky:
            logger.info(f"üèõÔ∏è Place urbaine d√©tect√©e!")
            return {'confidence': 0.80}
        
        return None
    
    def _remove_duplicates_and_sort(self, objects: List[Dict]) -> List[Dict]:
        """Supprime les doublons et trie par confiance"""
        seen = set()
        unique_objects = []
        for obj in objects:
            if obj['object'] not in seen:
                seen.add(obj['object'])
                unique_objects.append(obj)
        
        unique_objects.sort(key=lambda x: x['confidence'], reverse=True)
        return unique_objects
    
    def _detect_person_in_image(self, img_array: np.ndarray, colors: List[Dict]) -> Optional[Dict]:
        """D√©tecte la pr√©sence d'une personne dans l'image bas√© sur l'analyse visuelle"""
        logger.info("üë§ Analyse sp√©cifique pour d√©tection de personnes...")
        
        height, width = img_array.shape[:2]
        person_indicators = 0
        confidence_factors = []
        
        # 1. D√âTECTION DES TONS DE PEAU
        skin_colors = ['beige', 'rose', 'couleur mixte', 'marron', 'orange']
        for color in colors:
            color_name = color.get('name', '').lower()
            percentage = color.get('percentage', 0)
            
            # Analyse RGB pour tons de peau plus pr√©cise
            rgb = color.get('rgb', [0, 0, 0])
            if len(rgb) == 3:
                r, g, b = rgb
                # Algorithme de d√©tection de tons de peau
                if (r > 95 and g > 40 and b > 20 and 
                    max(r, g, b) - min(r, g, b) > 15 and 
                    abs(r - g) > 15 and r > g and r > b):
                    person_indicators += 2
                    confidence_factors.append(('skin_tone_rgb', 0.4))
                    logger.info(f"üé® Ton de peau d√©tect√© via RGB: {rgb}")
                    
                # Tons de peau par nom de couleur
                elif any(skin in color_name for skin in skin_colors) and percentage > 5:
                    person_indicators += 1
                    confidence_factors.append(('skin_tone_name', 0.2))
                    logger.info(f"üé® Ton de peau d√©tect√© via nom: {color_name}")
        
        # 2. ANALYSE DE FORME ET PROPORTIONS
        # V√©rifier les proportions typiques d'un portrait/personne
        aspect_ratio = width / height
        if 0.6 <= aspect_ratio <= 1.4:  # Format portrait ou carr√© typique des photos de personnes
            person_indicators += 1
            confidence_factors.append(('portrait_ratio', 0.15))
            logger.info(f"üìê Format portrait d√©tect√©: {aspect_ratio:.2f}")
        
        # 3. D√âTECTION DE COULEURS VESTIMENTAIRES
        clothing_colors = ['noir', 'blanc', 'gris', 'bleu', 'rouge', 'vert', 'jaune']
        clothing_detected = 0
        for color in colors:
            color_name = color.get('name', '').lower()
            percentage = color.get('percentage', 0)
            if color_name in clothing_colors and percentage > 10:
                clothing_detected += 1
        
        if clothing_detected >= 2:  # Au moins 2 couleurs de v√™tements
            person_indicators += 1
            confidence_factors.append(('clothing_colors', 0.2))
            logger.info(f"üëï Couleurs vestimentaires d√©tect√©es: {clothing_detected}")
        
        # 4. ANALYSE DE LA DISTRIBUTION DES COULEURS
        # Les photos de personnes ont souvent une distribution sp√©cifique
        if len(colors) >= 3:
            # V√©rifier s'il y a une couleur dominante (arri√®re-plan) et des couleurs secondaires (personne)
            main_color_pct = colors[0].get('percentage', 0)
            secondary_color_pct = colors[1].get('percentage', 0)
            
            if main_color_pct > 30 and secondary_color_pct > 10:
                person_indicators += 1
                confidence_factors.append(('color_distribution', 0.15))
                logger.info(f"üé® Distribution couleurs favorable: {main_color_pct}% / {secondary_color_pct}%")
        
        # 5. ANALYSE TEXTURE (bas√©e sur la variance des couleurs)
        color_variance = len([c for c in colors if c.get('percentage', 0) > 5])
        if color_variance >= 4:  # Variance de couleurs typique d'une personne (peau, cheveux, v√™tements, arri√®re-plan)
            person_indicators += 1
            confidence_factors.append(('color_variance', 0.1))
            logger.info(f"üåà Variance de couleurs d√©tect√©e: {color_variance}")
        
        # CALCUL DE LA CONFIANCE FINALE
        if person_indicators >= 2:  # Seuil minimum pour d√©tecter une personne
            base_confidence = min(0.85, person_indicators * 0.2)  # Base selon nombre d'indicateurs
            
            # Ajouter les facteurs de confiance sp√©cifiques
            bonus_confidence = sum(factor[1] for factor in confidence_factors)
            final_confidence = min(0.95, base_confidence + bonus_confidence)
            
            logger.info(f"‚úÖ Personne d√©tect√©e! Indicateurs: {person_indicators}, Confiance: {final_confidence:.3f}")
            logger.info(f"üìã Facteurs: {[f[0] for f in confidence_factors]}")
            
            return {
                'confidence': round(final_confidence, 3),
                'indicators': person_indicators,
                'factors': confidence_factors
            }
        
        logger.info(f"‚ùå Personne non d√©tect√©e. Indicateurs insuffisants: {person_indicators}")
        return None
    
    def detect_landmarks(self, image: Image.Image, threshold: float = 0.4) -> List[Dict]:
        """D√©tecte les monuments et lieux c√©l√®bres"""
        logger.info("üèõÔ∏è D√©tection de lieux...")
        
        # Mode simulation si CLIP n'est pas disponible - analyse bas√©e sur les couleurs
        if self.model is None:
            return self._simulate_landmark_detection(image)
        
        detected_locations = []
        
        try:
            # Tester seulement quelques landmarks populaires
            test_landmarks = {
                'Paris': ['Eiffel Tower', 'Arc de Triomphe', 'Louvre Museum'],
                'London': ['Big Ben', 'Tower Bridge', 'London Eye'],
                'New York': ['Statue of Liberty', 'Empire State Building', 'Brooklyn Bridge'],
                'Rome': ['Colosseum', 'Trevi Fountain', 'Vatican'],
                'Dubai': ['Burj Khalifa', 'Palm Jumeirah']
            }
            
            for city, landmarks in test_landmarks.items():
                # Cr√©er les prompts pour les monuments
                text_inputs = [f"a photo of {landmark}" for landmark in landmarks]
                
                inputs = self.processor(
                    text=text_inputs,
                    images=image,
                    return_tensors="pt",
                    padding=True
                ).to(self.device)
                
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    logits_per_image = outputs.logits_per_image
                    probs = logits_per_image.softmax(dim=1)
                
                # V√©rifier les monuments d√©tect√©s
                for landmark, prob in zip(landmarks, probs[0]):
                    confidence = prob.item()
                    if confidence > threshold:
                        detected_locations.append({
                            'landmark': landmark,
                            'city': city,
                            'confidence': round(confidence, 3)
                        })
            
            # Trier par confiance
            detected_locations.sort(key=lambda x: x['confidence'], reverse=True)
            return detected_locations[:3]
            
        except Exception as e:
            logger.error(f"Erreur d√©tection lieux: {e}")
            return []
    
    def _simulate_landmark_detection(self, image: Image.Image) -> List[Dict]:
        """Simulation de d√©tection de monuments bas√©e sur l'analyse de l'image"""
        logger.info("üé≠ Mode simulation - D√©tection de monuments bas√©e sur les couleurs et formes")
        
        # Analyser les couleurs pour d√©tecter des patterns
        colors = self.extract_dominant_colors(image, n_colors=8)
        
        # Convertir l'image en array pour analyse
        img_array = np.array(image)
        height, width = img_array.shape[:2]
        
        detected_locations = []
        monument_indicators = 0
        
        # Analyser les couleurs dominantes
        color_names = [c.get('name', '').lower() for c in colors[:5]]
        
        # Cr√©er un dictionnaire qui garde la PLUS GRANDE valeur pour chaque nom de couleur
        color_percentages = {}
        for c in colors[:5]:
            name = c.get('name', '').lower()
            percentage = c.get('percentage', 0)
            if name not in color_percentages or percentage > color_percentages[name]:
                color_percentages[name] = percentage
        
        # 1. D√âTECTION DE CIEL BLEU (monuments ext√©rieurs)
        has_blue_sky = any('bleu' in name and color_percentages.get(name, 0) > 10 
                          for name in color_percentages.keys())
        has_green = any('vert' in name and color_percentages.get(name, 0) > 8 
                       for name in color_percentages.keys())
        
        if has_blue_sky:
            monument_indicators += 1
            logger.info("‚òÅÔ∏è Ciel bleu d√©tect√© - monument ext√©rieur probable")
        
        # 2. D√âTECTION DE STRUCTURES (gris, marron, beige, noir)
        structure_colors = ['gris', 'marron', 'brun', 'noir', 'beige', 'orange']
        has_structure = any(name in structure_colors and color_percentages.get(name, 0) > 15 
                           for name in color_percentages.keys())
        if has_structure:
            monument_indicators += 2  # Poids plus important
            logger.info(f"üèõÔ∏è Structure d√©tect√©e - couleurs: {[n for n in color_names if n in structure_colors]}")
        
        # 3. PRIORIT√â HAUTE - ARC DE TRIOMPHE (Pierre blanche + PAS de verdure)
        has_white_stone = color_percentages.get('blanc', 0) > 15 or color_percentages.get('beige', 0) > 18
        
        if has_white_stone and has_blue_sky and not has_green:
            logger.info(f"üèõÔ∏è ARC DE TRIOMPHE d√©tect√©! (blanc:{color_percentages.get('blanc', 0):.1f}%, pas_vert:{not has_green})")
            detected_locations.append({
                'landmark': 'Arc de Triomphe',
                'city': 'Paris',
                'confidence': 0.88
            })
            return detected_locations
        
        # 4. PRIORIT√â HAUTE - TOUR EIFFEL (Structure m√©tallique + VERDURE)
        has_dark_structure = (color_percentages.get('marron', 0) > 10 or 
                             color_percentages.get('brun', 0) > 10 or
                             color_percentages.get('gris', 0) > 10)
        
        if has_dark_structure and has_blue_sky and has_green:
            logger.info(f"üóº TOUR EIFFEL d√©tect√©e! (structure_sombre:{has_dark_structure}, verdure:{has_green})")
            detected_locations.append({
                'landmark': 'Tour Eiffel',
                'city': 'Paris',
                'confidence': 0.92
            })
            return detected_locations
        
        # Tour Eiffel sans verdure visible (photo rapproch√©e)
        has_metallic = color_percentages.get('gris', 0) > 12 or color_percentages.get('marron', 0) > 12
        if has_metallic and has_blue_sky and not has_white_stone:
            logger.info(f"üóº TOUR EIFFEL probable (m√©tal:{has_metallic}, pas_blanc:{not has_white_stone})")
            detected_locations.append({
                'landmark': 'Tour Eiffel',
                'city': 'Paris',
                'confidence': 0.85
            })
            return detected_locations
        
        # 5. D√âTECTION DE SABLE/D√âSERT (pour pyramides)
        desert_colors = ['beige', 'orange', 'jaune', 'marron']
        has_desert = any(name in desert_colors and color_percentages.get(name, 0) > 20 
                        for name in color_percentages.keys())
        desert_percentage = sum(color_percentages.get(name, 0) for name in desert_colors)
        
        if has_desert and desert_percentage > 30:
            monument_indicators += 2
            logger.info(f"üèúÔ∏è D√©sert d√©tect√© ({desert_percentage:.1f}%) - possibles pyramides")
            
            # PYRAMIDES D'√âGYPTE
            if desert_percentage > 40:
                detected_locations.append({
                    'landmark': 'Pyramides de Gizeh',
                    'city': 'Le Caire, √âgypte',
                    'confidence': 0.85
                })
                logger.info("üî∫ PYRAMIDES D√âTECT√âES!")
                return detected_locations
        
        # 6. MONUMENT G√âN√âRIQUE (si assez d'indicateurs)
        if monument_indicators >= 3:
            logger.info(f"üèõÔ∏è Monument g√©n√©rique d√©tect√© ({monument_indicators} indicateurs)")
            detected_locations.append({
                'landmark': 'Monument historique',
                'city': 'Inconnu',
                'confidence': 0.60
            })
        
        return detected_locations
        
        try:
            # Tester seulement quelques landmarks populaires
            test_landmarks = {
                'Paris': ['Eiffel Tower', 'Arc de Triomphe'],
                'London': ['Big Ben', 'Tower Bridge'],
                'New York': ['Statue of Liberty', 'Empire State Building']
            }
            
            for city, landmarks in test_landmarks.items():
                # Cr√©er les prompts pour les monuments
                text_inputs = [f"a photo of {landmark}" for landmark in landmarks]
                
                inputs = self.processor(
                    text=text_inputs,
                    images=image,
                    return_tensors="pt",
                    padding=True
                ).to(self.device)
                
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    logits_per_image = outputs.logits_per_image
                    probs = logits_per_image.softmax(dim=1)
                
                # V√©rifier les monuments d√©tect√©s
                for landmark, prob in zip(landmarks, probs[0]):
                    confidence = prob.item()
                    if confidence > threshold:
                        detected_locations.append({
                            'landmark': landmark,
                            'city': city,
                            'confidence': round(confidence, 3)
                        })
            
            # Trier par confiance
            detected_locations.sort(key=lambda x: x['confidence'], reverse=True)
            
            logger.info(f"‚úÖ {len(detected_locations)} lieux d√©tect√©s")
            return detected_locations[:3]  # Top 3
            
        except Exception as e:
            logger.error(f"‚ùå Erreur d√©tection lieux: {e}")
            return []
    
    def extract_dominant_colors(self, image: Image.Image, n_colors: int = 5) -> List[Dict]:
        """Extrait les couleurs dominantes avec K-means"""
        logger.info("üé® Analyse des couleurs...")
        
        try:
            # Convertir en array numpy
            img_array = np.array(image)
            
            # Redimensionner pour acc√©l√©rer le traitement
            if img_array.shape[0] > 300 or img_array.shape[1] > 300:
                image_resized = image.copy()
                image_resized.thumbnail((300, 300))
                img_array = np.array(image_resized)
            
            # Reshape pour K-means
            pixels = img_array.reshape(-1, 3)
            
            # K-means clustering
            kmeans = KMeans(n_clusters=n_colors, random_state=42, n_init=10)
            kmeans.fit(pixels)
            
            colors = []
            percentages = np.bincount(kmeans.labels_) / len(pixels)
            
            for i, (color, percentage) in enumerate(zip(kmeans.cluster_centers_, percentages)):
                # Convertir en RGB entier
                rgb = [int(c) for c in color]
                hex_color = "#{:02x}{:02x}{:02x}".format(*rgb)
                
                colors.append({
                    'rgb': rgb,
                    'hex': hex_color,
                    'percentage': round(percentage * 100, 1),
                    'name': self._get_color_name(rgb)
                })
            
            # Trier par pourcentage d√©croissant
            colors.sort(key=lambda x: x['percentage'], reverse=True)
            
            logger.info(f"‚úÖ {len(colors)} couleurs dominantes extraites")
            return colors
            
        except Exception as e:
            logger.error(f"‚ùå Erreur analyse couleurs: {e}")
            return []
    
    def detect_emotions(self, image: Image.Image, threshold: float = 0.25) -> List[Dict]:
        """D√©tecte l'ambiance/√©motion de l'image"""
        logger.info("üòä D√©tection des √©motions...")
        
        # Mode simulation si CLIP n'est pas disponible
        if self.model is None:
            return self._simulate_emotion_detection(image)
        
        detected_emotions = []
        
        try:
            # Tester seulement quelques √©motions principales
            test_emotions = {
                'joyful': ['happy', 'bright', 'colorful'],
                'peaceful': ['calm', 'serene', 'quiet'],
                'dramatic': ['dark', 'intense', 'moody']
            }
            
            for emotion, keywords in test_emotions.items():
                # Cr√©er les prompts √©motionnels
                text_inputs = [f"a {keyword} photo" for keyword in keywords]
                
                inputs = self.processor(
                    text=text_inputs,
                    images=image,
                    return_tensors="pt",
                    padding=True
                ).to(self.device)
                
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    logits_per_image = outputs.logits_per_image
                    probs = logits_per_image.softmax(dim=1)
                
                # Calculer la confiance moyenne pour cette √©motion
                avg_confidence = probs[0].mean().item()
                
                if avg_confidence > threshold:
                    detected_emotions.append({
                        'emotion': emotion,
                        'confidence': round(avg_confidence, 3),
                        'keywords': keywords
                    })
            
            # Trier par confiance
            detected_emotions.sort(key=lambda x: x['confidence'], reverse=True)
            
            logger.info(f"‚úÖ {len(detected_emotions)} √©motions d√©tect√©es")
            return detected_emotions[:3]  # Top 3
            
        except Exception as e:
            logger.error(f"‚ùå Erreur d√©tection √©motions: {e}")
            return self._simulate_emotion_detection(image)
    
    def _simulate_emotion_detection(self, image: Image.Image) -> List[Dict]:
        """Simulation de d√©tection d'√©motions bas√©e sur les couleurs"""
        colors = self.extract_dominant_colors(image)
        emotions = []
        
        # Analyser les couleurs pour inf√©rer l'√©motion
        color_names = [c.get('name', '').lower() for c in colors[:3]]
        
        if any('rouge' in name or 'orange' in name for name in color_names):
            emotions.append({'emotion': 'energetic', 'confidence': 0.6, 'keywords': ['vibrant', 'warm']})
        if any('bleu' in name for name in color_names):
            emotions.append({'emotion': 'peaceful', 'confidence': 0.5, 'keywords': ['calm', 'cool']})
        if any('vert' in name for name in color_names):
            emotions.append({'emotion': 'natural', 'confidence': 0.7, 'keywords': ['nature', 'fresh']})
        
        return emotions[:2]
    
    def generate_description(self, image: Image.Image) -> str:
        """G√©n√®re une description basique de l'image"""
        logger.info("üìù G√©n√©ration de description...")
        
        # Mode simulation si CLIP n'est pas disponible
        if self.model is None:
            colors = self.extract_dominant_colors(image)
            if colors:
                main_color = colors[0].get('name', 'color√©e')
                return f"Image {main_color} avec {len(colors)} couleurs dominantes"
            return "Image analys√©e par Vision AI"
        
        try:
            # Prompts de description g√©n√©rique
            text_inputs = [
                "a beautiful photo",
                "an outdoor photo",
                "a colorful image"
            ]
            
            inputs = self.processor(
                text=text_inputs,
                images=image,
                return_tensors="pt",
                padding=True
            ).to(self.device)
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits_per_image = outputs.logits_per_image
                probs = logits_per_image.softmax(dim=1)
            
            # Prendre la description avec la plus haute probabilit√©
            best_idx = probs[0].argmax().item()
            best_description = text_inputs[best_idx]
            confidence = probs[0][best_idx].item()
            
            logger.info(f"‚úÖ Description g√©n√©r√©e: {best_description}")
            return f"{best_description} (confidence: {confidence:.3f})"
            
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration description: {e}")
            return "Image analysis completed"
    
    def _get_color_name(self, rgb: List[int]) -> str:
        """Donne un nom approximatif √† une couleur RGB"""
        r, g, b = rgb
        
        # Couleurs de base - conditions am√©lior√©es
        if r > 200 and g > 200 and b > 200:
            return "blanc"
        elif r < 50 and g < 50 and b < 50:
            return "noir"
        # D√âTECTION SP√âCIALE DES TONS DE PEAU
        elif (r > 95 and g > 40 and b > 20 and 
              max(r, g, b) - min(r, g, b) > 15 and 
              abs(r - g) > 15 and r > g and r > b):
            # Tons de peau clairs
            if r > 180 and g > 120:
                return "beige"
            # Tons de peau moyens
            elif r > 140 and g > 80:
                return "rose"
            # Tons de peau fonc√©s
            else:
                return "marron"
        elif r > 200 and g < 100 and b < 100:
            return "rouge"
        # Vert am√©lior√© - d√©tecte aussi les tons olives et verd√¢tres
        elif (g > r and g > b and g > 50) or (g > 80 and r < g + 30 and b < 100):
            return "vert"
        # Bleu am√©lior√© - d√©tecte toutes les nuances de bleu
        elif b > r and b > g and b > 80:
            return "bleu"
        elif r > 200 and g > 200 and b < 100:
            return "jaune"
        elif r > 200 and g < 100 and b > 200:
            return "magenta"
        elif r < 100 and g > 200 and b > 200:
            return "cyan"
        elif r > 150 and g > 100 and b < 100:
            return "orange"
        # Marron/brun pour les structures m√©talliques ET cheveux
        elif r > 60 and g > 40 and b < 80 and abs(r - g) < 50:
            return "marron"
        elif r > 150 and g < 150 and b > 150:
            return "violet"
        elif r > 100 and g > 100 and b > 100:
            return "gris"
        else:
            return "couleur mixte"


# Instance globale du service
vision_ai_service = VisionAIService()


def analyze_media_vision(image_path: Union[str, Path]) -> Dict:
    """
    Fonction utilitaire pour analyser une image
    
    Args:
        image_path: Chemin vers l'image
        
    Returns:
        Dict avec les r√©sultats d'analyse
    """
    return vision_ai_service.analyze_image(image_path)


# Test rapide si ex√©cut√© directement
if __name__ == "__main__":
    print("üß™ Test du service Vision AI")
    
    # Cr√©er une image de test simple
    test_image = Image.new('RGB', (100, 100), color='red')
    results = vision_ai_service.analyze_image(test_image)
    
    print("üìä R√©sultats du test:")
    print(f"  Objets d√©tect√©s: {len(results.get('detected_objects', []))}")
    print(f"  Lieux d√©tect√©s: {len(results.get('detected_locations', []))}")
    print(f"  Couleurs dominantes: {len(results.get('dominant_colors', []))}")
    print(f"  √âmotions d√©tect√©es: {len(results.get('detected_emotions', []))}")
    print("‚úÖ Service Vision AI op√©rationnel")